{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQtwn1n-JQyG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "915vbqPIJQyI",
        "outputId": "2b66ddf5-8fdb-4c1e-fb46-4763e6be8659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 5.38 s\n",
            "Wall time: 11.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# read dataframe\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "sample_submission = pd.read_csv('sample_solution.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quz934UkJQyI"
      },
      "outputs": [],
      "source": [
        "# prepare dataframe for modeling\n",
        "X = df_train.drop(columns=['id','claim']).copy()\n",
        "y = df_train['claim'].copy()\n",
        "\n",
        "test_data = df_test.drop(columns=['id']).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5I7_jFBJQyJ"
      },
      "outputs": [],
      "source": [
        "# feature-engineering\n",
        "def get_stats_per_row(data):\n",
        "    data['mv_row'] = data.isna().sum(axis=1)\n",
        "    data['min_row'] = data.min(axis=1)\n",
        "    data['std_row'] = data.std(axis=1)\n",
        "    return data\n",
        "\n",
        "X = get_stats_per_row(X)\n",
        "test_data = get_stats_per_row(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcSBrzo5JQyJ"
      },
      "outputs": [],
      "source": [
        "# get skewed features to impute median instead of mean\n",
        "from scipy.stats import skew\n",
        "\n",
        "def impute_skewed_features(data):\n",
        "    skewed_feat = data.skew()\n",
        "    skewed_feat = [*skewed_feat[abs(skewed_feat.values) > 1].index]\n",
        "\n",
        "    for feat in skewed_feat:\n",
        "        median = data[feat].median()\n",
        "        data[feat] = data[feat].fillna(median)\n",
        "\n",
        "    return data\n",
        "\n",
        "X = impute_skewed_features(X)\n",
        "test_data = impute_skewed_features(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFKt8XuJQyJ"
      },
      "outputs": [],
      "source": [
        "# create preprocessing pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='mean')),\n",
        "    ('scale', StandardScaler())\n",
        "])\n",
        "\n",
        "X = pd.DataFrame(columns=X.columns, data=pipeline.fit_transform(X))\n",
        "test_data = pd.DataFrame(columns=test_data.columns, data=pipeline.transform(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ8sb0MSJQyJ"
      },
      "outputs": [],
      "source": [
        "# helper functions\n",
        "def get_auc(y_true, y_hat):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_hat)\n",
        "    score = auc(fpr, tpr)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEJ1m9ZtJQyK"
      },
      "outputs": [],
      "source": [
        "# best params\n",
        "lgbm1_params = {\n",
        "    'metric' : 'auc',\n",
        "    'max_depth' : 3,\n",
        "    'num_leaves' : 7,\n",
        "    'n_estimators' : 5000,\n",
        "    'colsample_bytree' : 0.3,\n",
        "    'subsample' : 0.5,\n",
        "    'random_state' : 42,\n",
        "    'reg_alpha' : 18,\n",
        "    'reg_lambda' : 17,\n",
        "    'learning_rate' : 0.095,\n",
        "    'device' : 'gpu',\n",
        "    'objective' : 'binary'\n",
        "}\n",
        "\n",
        "lgbm2_params = {\n",
        "    'metric' : 'auc',\n",
        "    'objective': 'binary',\n",
        "    'n_estimators': 10000,\n",
        "    'random_state': 42,\n",
        "    'learning_rate': 0.095,\n",
        "    'subsample': 0.6,\n",
        "    'subsample_freq': 1,\n",
        "    'colsample_bytree': 0.4,\n",
        "    'reg_alpha': 10.0,\n",
        "    'reg_lambda': 1e-1,\n",
        "    'min_child_weight': 256,\n",
        "    'min_child_samples': 20,\n",
        "    'device' : 'gpu',\n",
        "    'max_depth' : 3,\n",
        "    'num_leaves' : 7\n",
        "}\n",
        "\n",
        "lgbm3_params = {\n",
        "    'metric' : 'auc',\n",
        "    'objective' : 'binary',\n",
        "    'device_type': 'gpu',\n",
        "    'n_estimators': 10000,\n",
        "    'learning_rate': 0.12230165751633416,\n",
        "    'num_leaves': 1400,\n",
        "    'max_depth': 8,\n",
        "    'min_child_samples': 3100,\n",
        "    'reg_alpha': 10,\n",
        "    'reg_lambda': 65,\n",
        "    'min_split_gain': 5.157818977461183,\n",
        "    'subsample': 0.5,\n",
        "    'subsample_freq': 1,\n",
        "    'colsample_bytree': 0.2\n",
        "}\n",
        "\n",
        "catb1_params = {\n",
        "    'eval_metric' : 'AUC',\n",
        "    'iterations': 15585,\n",
        "    'objective': 'CrossEntropy',\n",
        "    'bootstrap_type': 'Bernoulli',\n",
        "    'od_wait': 1144,\n",
        "    'learning_rate': 0.023575206684596582,\n",
        "    'reg_lambda': 36.30433203563295,\n",
        "    'random_strength': 43.75597655616195,\n",
        "    'depth': 7,\n",
        "    'min_data_in_leaf': 11,\n",
        "    'leaf_estimation_iterations': 1,\n",
        "    'subsample': 0.8227911142845009,\n",
        "    'task_type' : 'GPU',\n",
        "    'devices' : '0',\n",
        "    'verbose' : 0\n",
        "}\n",
        "\n",
        "catb2_params = {\n",
        "    'eval_metric' : 'AUC',\n",
        "    'depth' : 5,\n",
        "    'grow_policy' : 'SymmetricTree',\n",
        "    'l2_leaf_reg' : 3.0,\n",
        "    'random_strength' : 1.0,\n",
        "    'learning_rate' : 0.1,\n",
        "    'iterations' : 10000,\n",
        "    'loss_function' : 'CrossEntropy',\n",
        "    'task_type' : 'GPU',\n",
        "    'devices' : '0',\n",
        "    'verbose' : 0\n",
        "}\n",
        "\n",
        "xgb1_params = {\n",
        "    'eval_metric' : 'auc',\n",
        "    'lambda': 0.004562711234493688,\n",
        "    'alpha': 7.268146704546314,\n",
        "    'colsample_bytree': 0.6468987558386358,\n",
        "    'colsample_bynode': 0.29113878257290376,\n",
        "    'colsample_bylevel': 0.8915913499148167,\n",
        "    'subsample': 0.37130229826185135,\n",
        "    'learning_rate': 0.021671163563123198,\n",
        "    'grow_policy': 'lossguide',\n",
        "    'max_depth': 18,\n",
        "    'min_child_weight': 215,\n",
        "    'max_bin': 272,\n",
        "    'n_estimators': 10000,\n",
        "    'random_state': 0,\n",
        "    'use_label_encoder': False,\n",
        "    'objective': 'binary:logistic',\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'gpu_id': 0,\n",
        "    'predictor': 'gpu_predictor'\n",
        "}\n",
        "\n",
        "xgb2_params = dict(\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    subsample=0.5,\n",
        "    colsample_bytree=0.5,\n",
        "    learning_rate=0.01187431306013263,\n",
        "    n_estimators=10000,\n",
        "    n_jobs=-1,\n",
        "    use_label_encoder=False,\n",
        "    objective='binary:logistic',\n",
        "    tree_method='gpu_hist',\n",
        "    gpu_id=0,\n",
        "    predictor='gpu_predictor'\n",
        ")\n",
        "\n",
        "xgb3_params = {\n",
        "    'eval_metric': 'auc',\n",
        "    'objective': 'binary:logistic',\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'gpu_id': 0,\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'n_estimators': 10000,\n",
        "    'learning_rate': 0.01063045229441343,\n",
        "    'gamma': 0.24652519525750877,\n",
        "    'max_depth': 4,\n",
        "    'min_child_weight': 366,\n",
        "    'subsample': 0.6423040816299684,\n",
        "    'colsample_bytree': 0.7751264493218339,\n",
        "    'colsample_bylevel': 0.8675692743597421,\n",
        "    'lambda': 0,\n",
        "    'alpha': 10\n",
        "}\n",
        "\n",
        "hist1_params = {\n",
        "    'max_depth': 12,\n",
        "    'max_leaf_nodes': 175,\n",
        "    'min_samples_leaf': 13646,\n",
        "    'l2_regularization': 0.4559366258442665,\n",
        "    'random_state' : 666,\n",
        "    'max_iter' : 40000,\n",
        "    'learning_rate' : 0.025,\n",
        "    'validation_fraction' : 0.1,\n",
        "    'early_stopping' : True,\n",
        "    'n_iter_no_change' : 200,\n",
        "    'scoring' : \"roc_auc\",\n",
        "    'verbose' : 0\n",
        "}\n",
        "\n",
        "hist2_params = {    'l2_regularization': 3.5676998099580946e-05,\n",
        "    'learning_rate': 0.036266802532849,\n",
        "    'max_iter': 1000,\n",
        "    'max_depth': 20,\n",
        "    'max_bins': 234,\n",
        "    'min_samples_leaf': 10000,\n",
        "    'max_leaf_nodes': 48}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZqz_eEaJQyK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# create list[tuples] of base_models\n",
        "models = [\n",
        "    ('hist2', HistGradientBoostingClassifier(**hist2_params)),\n",
        "    ('hist1', HistGradientBoostingClassifier(**hist1_params)),\n",
        "\n",
        "    ('lgbm1', LGBMClassifier(**lgbm1_params)),\n",
        "#     ('lgbm2', LGBMClassifier(**lgbm2_params)),\n",
        "    ('lgbm3', LGBMClassifier(**lgbm3_params)),\n",
        "    ('catb1', CatBoostClassifier(**catb1_params)),\n",
        "    ('catb2', CatBoostClassifier(**catb2_params)),\n",
        "    ('xgb1', XGBClassifier(**xgb1_params)),\n",
        "    ('xgb2', XGBClassifier(**xgb2_params)),\n",
        "    ('xgb3', XGBClassifier(**xgb3_params))\n",
        "]\n",
        "\n",
        "# create dictionaries to store predictions\n",
        "oof_pred_tmp = dict()\n",
        "test_pred_tmp = dict()\n",
        "scores_tmp = dict()\n",
        "\n",
        "# create cv\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "for fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n",
        "    # create train, validation sets\n",
        "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
        "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
        "\n",
        "    # fit & predict all models on the same fold\n",
        "    for name, model in models:\n",
        "        if name not in scores_tmp:\n",
        "            oof_pred_tmp[name] = list()\n",
        "            oof_pred_tmp['y_valid'] = list()\n",
        "            test_pred_tmp[name] = list()\n",
        "            scores_tmp[name] = list()\n",
        "        if 'hist' in name:\n",
        "            model.fit(\n",
        "                X_train, y_train,\n",
        "    #             early_stopping_rounds=500,\n",
        "            )\n",
        "        else:\n",
        "            model.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_valid,y_valid)],\n",
        "    #             early_stopping_rounds=500,\n",
        "            )\n",
        "\n",
        "        # validation prediction\n",
        "        pred_valid = model.predict_proba(X_valid)[:,1]\n",
        "        score = get_auc(y_valid, pred_valid)\n",
        "\n",
        "        scores_tmp[name].append(score)\n",
        "        oof_pred_tmp[name].extend(pred_valid)\n",
        "\n",
        "        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n",
        "        print('--'*20)\n",
        "\n",
        "        # test prediction\n",
        "        y_hat = model.predict_proba(test_data)[:,1]\n",
        "        test_pred_tmp[name].append(y_hat)\n",
        "\n",
        "    # store y_validation for later use\n",
        "    oof_pred_tmp['y_valid'].extend(y_valid)\n",
        "\n",
        "# print overall validation scores\n",
        "for name, model in models:\n",
        "    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n",
        "    print('::'*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njXlkMB0JQyL"
      },
      "outputs": [],
      "source": [
        "# create df with base predictions on test_data\n",
        "base_test_predictions = pd.DataFrame(\n",
        "    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1)\n",
        "    for name in test_pred_tmp.keys()}\n",
        ")\n",
        "\n",
        "# save csv checkpoint\n",
        "base_test_predictions.to_csv('./base_test_predictions.csv', index=False)\n",
        "\n",
        "# create simple average blend\n",
        "base_test_predictions['simple_avg'] = base_test_predictions.mean(axis=1)\n",
        "\n",
        "# create submission file with simple blend average\n",
        "simple_blend_submission = sample_submission.copy()\n",
        "simple_blend_submission['claim'] = base_test_predictions['simple_avg']\n",
        "simple_blend_submission.to_csv('./simple_blend_submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3huYnLzJQyL"
      },
      "outputs": [],
      "source": [
        "# create training set for meta learner based on the oof_predictions of the base models\n",
        "oof_predictions = pd.DataFrame(\n",
        "    {name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()}\n",
        ")\n",
        "\n",
        "# save csv checkpoint\n",
        "oof_predictions.to_csv('./oof_predictions.csv', index=False)\n",
        "\n",
        "# get simple blend validation score\n",
        "y_valid = oof_predictions['y_valid'].copy()\n",
        "y_hat_blend = oof_predictions.drop(columns=['y_valid']).mean(axis=1)\n",
        "score = get_auc(y_valid, y_hat_blend)\n",
        "\n",
        "print(f\"Overall Validation Score | Simple Blend: {score}\")\n",
        "print('::'*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YNEWB_UJQyL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# prepare meta_training set\n",
        "X_meta = oof_predictions.drop(columns=['y_valid']).copy()\n",
        "y_meta = oof_predictions['y_valid'].copy()\n",
        "test_meta = base_test_predictions.drop(columns=['simple_avg']).copy()\n",
        "\n",
        "meta_pred_tmp = []\n",
        "scores_tmp = []\n",
        "\n",
        "# create cv\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "for fold, (idx_train, idx_valid) in enumerate(kf.split(X_meta, y_meta)):\n",
        "    # create train, validation sets\n",
        "    X_train, y_train = X_meta.iloc[idx_train], y_meta.iloc[idx_train]\n",
        "    X_valid, y_valid = X_meta.iloc[idx_valid], y_meta.iloc[idx_valid]\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # validation prediction\n",
        "    pred_valid = model.predict_proba(X_valid)[:,1]\n",
        "    score = get_auc(y_valid, pred_valid)\n",
        "    scores_tmp.append(score)\n",
        "\n",
        "    print(f\"Fold: {fold + 1} Score: {score}\")\n",
        "    print('--'*20)\n",
        "\n",
        "    # test prediction based on oof_set\n",
        "    y_hat = model.predict_proba(test_meta)[:,1]\n",
        "    meta_pred_tmp.append(y_hat)\n",
        "\n",
        "# print overall validation scores\n",
        "print(f\"Overall Validation Score | Meta: {np.mean(scores_tmp)}\")\n",
        "print('::'*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr0g2Qd_JQyL"
      },
      "outputs": [],
      "source": [
        "# average meta predictions over each fold\n",
        "meta_predictions = np.mean(np.column_stack(meta_pred_tmp), axis=1)\n",
        "\n",
        "# create submission file\n",
        "stacked_submission = sample_submission.copy()\n",
        "stacked_submission['claim'] = meta_predictions\n",
        "stacked_submission.to_csv('./stacked_submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3te_1LZJQyL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
